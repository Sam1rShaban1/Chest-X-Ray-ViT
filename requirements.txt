# Fine-tuning ViT for NIH Chest X-ray Classification on GCP TPUs\n\nThis project demonstrates how to fine-tune a Vision Transformer (ViT) model for multi-label classification of chest X-ray images from the NIH Chest X-ray dataset, leveraging Google Cloud Platform (GCP) resources, specifically Google Compute Engine (GCE) and Cloud TPUs (Tensor Processing Units).\n\n## Project Structure\n\n*   `ViT-Training.py`: The main Python script. It handles:\n    *   Loading data from Google Cloud Storage (GCS).\n    *   Preprocessing images.\n    *   Creating PyTorch datasets and data loaders.\n    *   Loading a pre-trained ViT model.\n    *   Fine-tuning the model for the chest X-ray classification task.\n    *   Evaluating the model on a test set.\n    *   Saving the best model.\n    *   Plotting training metrics (loss, AUROC).\n\n*   `requirements.txt`: Lists the Python packages needed for the project.  \n\n*   `README.md` (this file): Provides an overview of the project, setup instructions, and usage information.\n\n## Prerequisites\n\n1.  **Google Cloud Account:** You need a Google Cloud account with billing enabled.\n2.  **Google Cloud Project:** You must have a GCP project set up. Enable the Cloud TPU API, the Compute Engine API, and the Cloud Storage API.\n3.  **Google Cloud Storage (GCS) Bucket:** You will need a GCS bucket to store the NIH Chest X-ray dataset. The dataset should be structured with `Data_Entry_2017.csv`, `BBox_List_2017.csv`, `train_val_list.txt`, `test_list.txt` at the root.  You can create subfolders for organization if desired, and update the GCS paths accordingly.\n4.  **NIH Chest X-ray Dataset:** The NIH Chest X-ray dataset.  Upload the required files to your GCS bucket.\n\n5.  **Google Compute Engine (GCE) TPU VM:** Create a GCE TPU VM (or use an existing one). The VM should:\n    *   Be in the same region as your GCS bucket (for best performance).\n    *   Be running a recent TPU VM image. (e.g., `tpu-vm-pt-2.0` or similar).\n    *   Have a service account configured with the following IAM roles:\n        *   `Storage Object Admin`: On the bucket holding the NIH Chest X-ray data. This grants the VM read/write access to your data in GCS.\n        *   `Cloud TPU Admin`\n        *   `Compute Instance Admin (v1)`\n6.  **`gcloud` Command-Line Tool:** Install and configure the Google Cloud SDK (`gcloud`) on your local machine.  You'll use it to transfer the Python script to the VM.\n\n## Setup and Usage\n\n1.  **Create the `requirements.txt` file.** Create the `requirements.txt` file with the dependencies. (See below for the recommended content).\n\n```\n# requirements.txt\n# This file lists the Python packages needed for the train_vit_nih_tpu.py script.\n# Install via: pip install -r requirements.txt --no-deps  (AFTER step 2).\n\n# Core PyTorch and XLA libraries (install with special command - DO NOT include here):  These are handled separately, as per the TPU documentation.\n# torch~=2.5.0\n# torchvision~=0.19.0\n# torch_xla[tpu]~=2.5.0\n\n# Other Core Deep Learning Libraries\ntransformers\naccelerate\nbitsandbytes\n\n# Data Handling and Utility Libraries\nnumpy\npandas\nPillow\nmatplotlib\nscikit-learn\ngoogle-cloud-storage\ntqdm\n```\n\n2.  **Install Python packages on the GCE TPU VM.** SSH into your TPU VM (using `gcloud compute ssh <VM_NAME> --zone=<VM_ZONE> --project=<YOUR_GCP_PROJECT_ID>`) and use the following commands, in the *exact order*: First, install the PyTorch/XLA libraries, then install other libraries.\n    *   First, the PyTorch/XLA installations using the special command:\n\n        ```bash\n        pip uninstall -y torch torchvision torchaudio torch_xla transformers accelerate bitsandbytes  # Clean Uninstall (if needed).\n        pip install torch~=2.5.0 torchvision~=0.19.0 torch_xla[tpu]~=2.5.0 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n        ```\n\n    *   Second, the remaining dependencies from `requirements.txt`:\n\n        ```bash\n        pip install -r ~/requirements.txt --no-deps\n        ```\n\n    *   (Optional) Verify the installation:\n\n        ```bash\n        python3 -c \"import torch; import torch_xla; print(f'PyTorch: {torch.__version__}, Torch/XLA: {torch_xla.__version__}');\"  # Check version and if torch_xla is importable.\n        ```\n\n3.  **Upload the Script to Your GCE VM:**  Using `gcloud compute scp` on your local machine:\n    ```bash\n    gcloud compute scp <PATH_TO_LOCAL_SCRIPT>/ViT-Training.py <VM_NAME>:~/ --zone=<VM_ZONE> --project=<YOUR_GCP_PROJECT_ID>\n    ```\n    Replace `<PATH_TO_LOCAL_SCRIPT>`, `<VM_NAME>`, `<VM_ZONE>`, and `<YOUR_GCP_PROJECT_ID>` with the correct values.\n\n4.  **Run the Python Script:**  Connect to your GCE VM via SSH and run the Python script:\n    ```bash\n    python3 ~/ViT-Training.py\n    ```\n\n    *   Monitor the training progress in the SSH terminal. Training logs, checkpoints, and plots will be saved to the `OUTPUT_DIR` (e.g., `~/vit_finetune_results/`) on the VM.\n\n## Configuration Details (Inside `ViT-Training.py`) - Important!**\n\n*   **`GCP_PROJECT_ID`**: Your Google Cloud Project ID. *Replace the placeholder with your actual project ID.*\n*   **`GCS_BUCKET_NAME`**: The name of your GCS bucket containing the dataset. *Replace the placeholder with your actual bucket name.*\n*   **`GCS_IMAGE_BASE_PREFIX`**: The prefix within the bucket where your image files (e.g., `images_001/images/000.png`) are located. If images are directly at the root of the bucket, set this to `""`. Adjust this path based on your GCS image storage structure. This is where the code looks for image files. If you uploaded files to the bucket at the root level, then this is equal to `""`. Otherwise, set it to the path.\n*   **`GCS_BBOX_CSV_PATH`, `GCS_DATA_ENTRY_CSV_PATH`, `GCS_TRAIN_VAL_LIST_PATH`, `GCS_TEST_LIST_PATH`**: The paths to your data entry and split files.  These should be the full paths *relative to the GCS bucket's root*, e.g.,  `BBox_List_2017.csv`. *Replace the placeholder with your actual file paths.*\n*   **`MODEL_NAME`**:  The name of the pre-trained ViT model to use. (e.g., `'google/vit-base-patch16-384'`)\n*   **`IMG_SIZE`**:  The input image size (384x384).  Must match the model's expected input size.\n*   **`VIT_MEAN`, `VIT_STD`**: The mean and standard deviation values for image normalization (ImageNet). These are specific to the pre-trained ViT model you choose.\n*   **`BATCH_SIZE_PER_CORE`**: The batch size *per* TPU core.  For a `v3-8` TPU (8 cores), a `BATCH_SIZE_PER_CORE` of 16 results in an effective global batch size of 128.  Adjust to fit your TPU memory.  Experiment.  Increase the value and if it stops working, lower the value. \n*   **`LEARNING_RATE`**: The learning rate for fine-tuning.  (e.g., `2e-4`)\n*   **`WEIGHT_DECAY`**: Weight decay for the optimizer.  (e.g., `0.01`)\n*   **`NUM_EPOCHS`**: The number of training epochs. (e.g., `4`)\n*   **`NUM_WORKERS`**: The number of data loader workers (CPU processes) *per* DataLoader.  For TPU VMs, this can be higher to feed the TPU with data. (e.g., `8`).  This can be adjusted based on the number of CPU cores available. Try to set this value to be as high as possible to provide the most efficient data loading. \n*   `OUTPUT_DIR`: The directory where the training logs, metrics, and model checkpoints will be saved *on the VM*.  (e.g., `~/vit_finetune_results/`). This will be relative to the user directory that is running the script, unless the full path is given.  Make sure to replace this with a directory that the user has write access to. \n\n## Key Code Components\n\n*   **Data Loading:** Reads the images and metadata directly from GCS, using `google-cloud-storage`.  It leverages the pre-built GCS blob map to make it efficient.  The NIHChestDataset class handles loading images, creating bboxes, and processing the labels.\n*   **BBox Cropping:** Uses the bounding box information from `BBox_List_2017.csv` to crop the ROI. The `crop_and_pad_from_bbox` and `pad_to_square` functions handle the cropping and padding of the bounding box data.\n*   **ViT Preprocessing:** Applies the necessary image transformations (resizing, conversion to RGB, normalization) to prepare images for the ViT model. `roi_preprocess_transforms` takes care of this.\n*   **TPU-Aware Training:** Uses Hugging Face `Trainer` with PyTorch/XLA to handle training on the TPU. It uses `xm.xla_device()` to define the device and includes the necessary XLA primitives.\n\n## Troubleshooting\n\n*   **403 Access Denied Errors:**\n    *   **Permissions:** Double-check the service account used by the GCE VM has the `Storage Object Admin` role *on your GCS bucket*.  This is almost always the cause. Verify in the GCP Console. \n    *   **GCS Path:** Verify that the `GCS_BUCKET_NAME`, `GCS_IMAGE_BASE_PREFIX`, and GCS file paths in `ViT-Training.py` are correct. \n*   **`ModuleNotFoundError: No module named 'torch_xla'`:**\n    *   **Installation:** The most common problem.  Carefully follow the installation instructions in this README and the comments in the code. It is *crucial* that `torch`, `torchvision`, and `torch_xla` are installed *in the correct order* on the TPU VM, from the right PyPI index using the `install_libraries.sh`.  Verify this by running `python3 -c \"import torch; import torch_xla; print(f'PyTorch: {torch.__version__}, Torch/XLA: {torch_xla.__version__}');\"` on the VM.\n*   **Slow Data Loading:**\n    *   Ensure `NUM_WORKERS` is set to an appropriate value (e.g., the number of CPU cores on your VM) and that you have built a blob map. Check the loading of the data by looking at the print statements. You want the training to be as fast as possible, and often the data loading can be the bottle neck of training. \n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n",
  "requirements": "requirements.txt content:\n\n# requirements.txt\n# This file lists the Python packages needed for the train_vit_nih_tpu.py script.\n# Install via: pip install -r requirements.txt --no-deps  (AFTER step 2).\n\n# Core PyTorch and XLA libraries (install with special command - DO NOT include here):  These are handled separately, as per the TPU documentation.\n# torch~=2.5.0\n# torchvision~=0.19.0\n# torch_xla[tpu]~=2.5.0\n\n# Other Core Deep Learning Libraries\ntransformers\naccelerate\nbitsandbytes\n\n# Data Handling and Utility Libraries\nnumpy\npandas\nPillow\nmatplotlib\nscikit-learn\ngoogle-cloud-storage\ntqdm